{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200217072606-0000\nKERNEL_ID = 31a583ee-517f-4188-b0c4-27b958f16f59\nCloning into 'HMP_Dataset'...\nremote: Enumerating objects: 865, done.\u001b[K\nremote: Total 865 (delta 0), reused 0 (delta 0), pack-reused 865\u001b[K\nReceiving objects: 100% (865/865), 1010.96 KiB | 0 bytes/s, done.\nChecking out files: 100% (848/848), done.\n"
                }
            ],
            "source": "!git clone https://github.com/wchill/HMP_Dataset.git"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Brush_teeth\tDrink_glass  Liedown_bed  Sitdown_chair  final.py\r\nClimb_stairs\tEat_meat     MANUAL.txt   Standup_chair  impdata.py\r\nComb_hair\tEat_soup     Pour_water   Use_telephone\r\nDescend_stairs\tGetup_bed    README.txt   Walk\r\n"
                }
            ],
            "source": "!ls HMP_Dataset"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Accelerometer-2011-03-24-10-24-39-climb_stairs-f1.txt\r\nAccelerometer-2011-03-24-10-25-44-climb_stairs-f1.txt\r\nAccelerometer-2011-03-29-09-55-46-climb_stairs-f1.txt\r\nAccelerometer-2011-04-05-18-21-22-climb_stairs-f1.txt\r\nAccelerometer-2011-04-05-18-32-29-climb_stairs-f1.txt\r\nAccelerometer-2011-04-11-11-44-35-climb_stairs-f1.txt\r\nAccelerometer-2011-04-11-11-57-50-climb_stairs-f1.txt\r\nAccelerometer-2011-04-11-11-58-30-climb_stairs-f1.txt\r\nAccelerometer-2011-05-30-08-21-38-climb_stairs-f1.txt\r\nAccelerometer-2011-05-30-08-30-37-climb_stairs-f1.txt\r\n"
                }
            ],
            "source": "!ls HMP_Dataset/Climb_stairs | head -n 10"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "# define the schema for the dataset\n\nfrom pyspark.sql.types import StructType, StructField, IntegerType\n\nschema = StructType([\n                StructField(\"x\", IntegerType(), True),\n                StructField(\"y\", IntegerType(), True),\n                StructField(\"z\", IntegerType(), True)                \n                    ])"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "['.git',\n '.idea',\n 'Brush_teeth',\n 'Climb_stairs',\n 'Comb_hair',\n 'Descend_stairs',\n 'Drink_glass',\n 'Eat_meat',\n 'Eat_soup',\n 'Getup_bed',\n 'Liedown_bed',\n 'MANUAL.txt',\n 'Pour_water',\n 'README.txt',\n 'Sitdown_chair',\n 'Standup_chair',\n 'Use_telephone',\n 'Walk',\n 'final.py',\n 'impdata.py']"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import os\nfilelist = os.listdir('HMP_Dataset')\nfilelist"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "['Brush_teeth',\n 'Climb_stairs',\n 'Comb_hair',\n 'Descend_stairs',\n 'Drink_glass',\n 'Eat_meat',\n 'Eat_soup',\n 'Getup_bed',\n 'Liedown_bed',\n 'Pour_water',\n 'Sitdown_chair',\n 'Standup_chair',\n 'Use_telephone']"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# select the ones with an \"_\"\n\nfilelist_filtered = [f for f in filelist if '_' in f]\nfilelist_filtered"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": "df = None\n\nfrom pyspark.sql.functions import lit\n\nfor category in filelist_filtered:\n    data_files = os.listdir('HMP_Dataset/'+category) # subdirectores\n    \n    # iterate over the files in the subdirectories and create a temp dataframe\n    for data_file in data_files:\n        #print(data_file)\n        temp_df = spark.read\\\n                    .option('header', 'false')\\ # since there are no headers\n                    .option('delimiter', ' ')\\ # space separated\n                    .csv('HMP_Dataset/'+category+'/'+data_file, schema= schema)\n        \n        \n    # adding literals or strings to the dataframe to identify the source, import the 'lit' function\n        \n        # here 'class' denotes the folder name where the data comes from\n        temp_df = temp_df.withColumn('class', lit(category))\n        \n        # similary do it for the 'source file'\n        temp_df = temp_df.withColumn('source', lit(data_file))\n        \n        if df is None: # first iteration\n            df = temp_df\n        else:\n            df = df.union(temp_df) # append the data vertically      \n        \n        "
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+\n|  x|  y|  z|      class|              source|\n+---+---+---+-----------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|\n+---+---+---+-----------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "df.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+----------+\n|  x|  y|  z|      class|              source|classIndex|\n+---+---+---+-----------+--------------------+----------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|\n+---+---+---+-----------+--------------------+----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# Data transformation\n# Create integer represenation of the class\n# StringIndexer is an estimator, as it needs to parse through the entire data and remember what integer is assigned to each category string\n# Hence StringIndexer can remember the state\n\nfrom pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol='class', outputCol = 'classIndex')\n\n# use the fit() and transform method\n\nindexed = indexer.fit(df).transform(df)\nindexed.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+----------+--------------+\n|  x|  y|  z|      class|              source|classIndex|   categoryVec|\n+---+---+---+-----------+--------------------+----------+--------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|\n+---+---+---+-----------+--------------------+----------+--------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# Data tranformation: Step 2\n# One Hot encoding of the data\n# One hot representation in Apache Spark is not the typical one hot representation\n# so here (12,[5],[1.0]), means:\n    # total 12 elements\n    # at position [5], there is 1.0\n\n\nfrom pyspark.ml.feature import OneHotEncoder\n\nencoder = OneHotEncoder(inputCol='classIndex', outputCol='categoryVec')\n#encoded = encoder.fit(indexed).transform(indexed)\n# fit() method not available for OnehotEncoder as it is not an 'estimator', only a 'transformer'\n\nencoded = encoder.transform(indexed)\nencoded.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+----------+--------------+----------------+\n|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# Data transformation - Step 3\n# Convert the (x, y, z) representation into a vectorized form since Spark ML can work only on vectors\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\n# the column 'features' will be used by the ML algorithm\nvectorAssembler =VectorAssembler(inputCols = ['x', 'y', 'z'], outputCol = 'features')\n\nfeatures_vectorized = vectorAssembler.transform(encoded)\n\nfeatures_vectorized.show()\n\n# the column 'features' created is an 'Apache Spark feature Object'"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# Data transformation - Step 4\n# Data normalization if required\n\nfrom pyspark.ml.feature import Normalizer\n\nnormalizer = Normalizer(inputCol='features', outputCol='features_norm', p=1.0)\nnormalized = normalizer.transform(features_vectorized)\nnormalized.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n|  x|  y|  z|      class|              source|classIndex|   categoryVec|        features|       features_norm|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 49| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 22| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n| 21| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n| 22| 51| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n| 20| 50| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n| 22| 52| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n| 22| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n| 22| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n| 21| 51| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|\n| 20| 50| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 21| 49| 33|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n| 20| 51| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|\n| 18| 49| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|\n| 19| 48| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|\n| 16| 53| 34|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|\n| 18| 52| 35|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|\n| 18| 51| 32|Brush_teeth|Accelerometer-201...|       5.0|(12,[5],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|\n+---+---+---+-----------+--------------------+----------+--------------+----------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# Create Pipelines\n\nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\n\nmodel = pipeline.fit(df)\nprediction = model.transform(df)\nprediction.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+----------+--------------+--------------------+\n|classIndex|   categoryVec|       features_norm|\n+----------+--------------+--------------------+\n|       5.0|(12,[5],[1.0])|[0.20754716981132...|\n|       5.0|(12,[5],[1.0])|[0.20754716981132...|\n|       5.0|(12,[5],[1.0])|[0.20183486238532...|\n|       5.0|(12,[5],[1.0])|[0.20183486238532...|\n|       5.0|(12,[5],[1.0])|[0.19626168224299...|\n|       5.0|(12,[5],[1.0])|[0.20560747663551...|\n|       5.0|(12,[5],[1.0])|[0.19047619047619...|\n|       5.0|(12,[5],[1.0])|[0.20370370370370...|\n|       5.0|(12,[5],[1.0])|[0.20754716981132...|\n|       5.0|(12,[5],[1.0])|[0.20370370370370...|\n|       5.0|(12,[5],[1.0])|[0.2,0.4857142857...|\n|       5.0|(12,[5],[1.0])|[0.19230769230769...|\n|       5.0|(12,[5],[1.0])|[0.20388349514563...|\n|       5.0|(12,[5],[1.0])|[0.20388349514563...|\n|       5.0|(12,[5],[1.0])|[0.18867924528301...|\n|       5.0|(12,[5],[1.0])|[0.17821782178217...|\n|       5.0|(12,[5],[1.0])|[0.18811881188118...|\n|       5.0|(12,[5],[1.0])|[0.15533980582524...|\n|       5.0|(12,[5],[1.0])|[0.17142857142857...|\n|       5.0|(12,[5],[1.0])|[0.17821782178217...|\n+----------+--------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# Now drop unnecessary columns\n\n# Another way\n#df_train = prediction.drop('x').drop('y').drop('z').drop('class')...\ncolumns_to_drop = ['x', 'y', 'z', 'class', 'source', 'features']\n\ndf_train = prediction.drop(*columns_to_drop)\ndf_train.show()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}